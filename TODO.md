# ðŸ“Â `TODO.md` â€” *Selfâ€‘Specâ€¯Llamaâ€‘4Â Sprint*  

Put this file at the root of your repo. Work topâ€‘toâ€‘bottom; each checkbox is **one small, mergeâ€‘able commit** (aim â‰¤â€¯200â€¯LOC).  
Everything assumes **condaâ€¯envâ€¯`llama4`**, **vLLMâ€¯`nightly`** fork, **H100â€¯80â€¯GB**, **Llamaâ€‘4â€¯Scoutâ€‘17Bâ€‘16Eâ€¯(INT4)** weights in `./model`.

---

## ðŸ“¦Â MilestoneÂ 0 â€” Baseline & Bench Harness
- [ ] **M0â€‘01 â€“ clone + env**  
  ```bash
  git clone https://github.com/vllm-project/vllm.git && cd vllm && git checkout nightly
  pip install -e ".[dev]"
  ```
- [ ] **M0â€‘02 â€“ pull model** (`huggingface_hub.snapshot_download`)
- [ ] **M0â€‘03 â€“ add `scripts/bench.sh`** (wrap `benchmark_engine.py`, save JSON â†’ `results/`)
- [ ] **M0â€‘04 â€“ run baseline**  
  ```bash
  # micro sanity first (8p/16g) catches shape mismatches fast
  ./scripts/bench.sh baseline_sanity 8 16 --seed 0
  # full run for headline numbers
  ./scripts/bench.sh baseline 32 128 --seed 0
  # expect ~400 tok/s, record in README
  ```

---

## â˜ï¸Â InfraÂ BootstrapÂ â€”Â SkyPilotÂ +Â GCP Spot

> Goal: oneâ€‘line launch, autoâ€‘retry after preâ€‘empt, code sync, VS Code SSH, cost guard.

| ID | task | done |
|----|------|------|
|INFâ€‘01|`pip install -U skypilot && sky check` (laptop)|âœ“|
|INFâ€‘02|Create **GCS bucket** `gs://selfspec-ckpt` (region = usâ€‘central2)|âœ“|
|INFâ€‘03|Add `infra/selfspec.yaml`<br><sup>```yaml\nresources:\n  cloud: gcp\n  region: us-central2\n  accelerators: H100:1\n  use_spot: true\n  disk_size: 80\nfile_mounts:\n  /workspace: .\nstorage_mounts:\n  /ckpt: gs://selfspec-ckpt\nsetup: |\n  conda env create -f env/environment.yml -n llama4 || true\nrun: |\n  conda activate llama4\n  python main.py --resume /ckpt/latest.pt\n```</sup>|âœ“|
|INFâ€‘04|`sky launch -c llama4 infra/selfspec.yaml --env HF_TOKEN=$HF_TOKEN --env WANDB_API_KEY=$WANDB_API_KEY --idle-minutes-to-autostop 30` â†’ VM online|âœ“|
|INFâ€‘05|`sky ssh dev -- -L 10022:localhost:22` then add<br>```\nHost skydev\n  HostName 127.0.0.1\n  Port 10022\n  User sky\n``` to `~/.ssh/config`; open VS Code **Remoteâ€‘SSH: skydev**|âœ“|

---


That's everything missing: **Spot H100 lifecycle wrapped, automatic file sync, VSCode SSH, cost cap, and CUDA ready**. Integrate, commit, and hack away.
## âš™ï¸Â MilestoneÂ 1Â â€”Â Layerâ€‘Skip Draft Path
- [ ] **M1â€‘05 â€“ env flag `LAYERSKIP`**  
  *Patch* `vllm/engine/spec_decode.py` â†’ expose `--layerskip` CLI flag (plumb into vLLM arg parser), if `layerskip>0` call `model.forward_partial(stop_layer=layerskip)`
- [ ] **M1â€‘06 â€“ quick sweep Lâˆˆ{8,12,16}** (`bench.sh skip$L`); commit CSV
  ```bash
  # cache hiddenâ€‘state tensor once; vary proj only â†’ <30â€¯s total
  for L in 8 12 16; do ./scripts/bench.sh skip$L 32 128 --layerskip $L --seed 0; done
  ```
- [ ] **M1â€‘07 â€“ add Nsight script** (`scripts/profile.sh`)

---

## ðŸ§©Â MilestoneÂ 2Â â€”Â Tiny Linear EEâ€‘Head (LSQ Fit)
- [ ] **M2â€‘08 â€“ `src/fit_lsq.py`**  
  - collect 50â€¯k `(h12, next_id)` pairs from ShareGPT  
  - solve `HÂ·Wâ‰ˆY` via `np.linalg.lstsq` (fit LSQ in fp32 **CPU**, ~20â€¯s)
  - *optional* 2â€‘min GPU SGD (1â€¯epoch) lifts acc +4â€¯pp
  - save `{"weight": W}` â†’ `ee_head/ee_layer12_lsq.pt`
- [ ] **M2â€‘09 â€“ wire projection**  
  *Patch* draft path: if `layerskip>0` load weight once, compute `logits = h12 @ W.T`
- [ ] **M2â€‘10 â€“ bench `skip12_lsq`**; update table in README  
  *target*: â‰¥â€¯600Â tok/s, Î”PPLâ€¯â‰¤â€¯1.5

---

## ðŸš¦Â MilestoneÂ 3Â â€”Â Selfâ€‘Spec Loop + Maxâ€‘Prob Gate
- [ ] **M3â€‘11 â€“ implement longestâ€‘prefix verify** (one helper in `spec_decode.py`)
  # gate fn chosen **once** outside hot loop â†’ zero branch misâ€‘pred
- [ ] **M3â€‘12 â€“ env flags**  
  - `EE_TAU` (float) â†’ maxâ€‘prob threshold  
  - `EE_GATE=maxp`
- [ ] **M3â€‘13 â€“ sweep `TAUâˆˆ{0.30,0.35,0.40}`**; log acceptâ€‘rate (accepted/total) â†’ `results/accept.tsv`, tok/s
- [ ] **M3â€‘14 â€“ update `results/summary.csv`**

---

## ðŸ“ŠÂ MilestoneÂ 4Â â€”Â Varâ€‘Entropy Gate
- [ ] **M4â€‘15 â€“ add `gating.py.varentropy(logits)`**
- [ ] **M4â€‘16 â€“ new env `EE_GATE=varH`, `EE_TAU_V`**
- [ ] **M4â€‘17 â€“ sweep `EE_TAU_Vâˆˆ{4,5,6}`**; pick Ï„V=5 (target â‰¥50â€¯% accept, Î”PPL â‰¤ 0.18) â†’ aim â‰¥â€¯800Â tok/s
- [ ] **M4â€‘18 â€“ notebook `notebooks/roc_gate.ipynb`** (ROC of maxâ€‘prob vs varH)

---

## ðŸ”¬Â MilestoneÂ 5Â â€”Â Quality & Latency Validation
- [ ] **M5â€‘19 â€“ add `scripts/ppl_eval.sh`** using `lm-eval-harness` on ARC, HellaSwag
  `harness --num_fewshot 0 --limit 100` # keeps run <15â€¯min/GPU
- [ ] **M5â€‘20 â€“ add `scripts/locust_load.py`** (32Â users, stream 128Â tokens)
  `locust -f ... --disable-log-stats` # p99 stable
- [ ] **M5â€‘21 â€“ run 10â€¯min soak, export `locust_stats.csv`**
- [ ] **M5â€‘22 â€“ grafana dash screenshot**

---

## ðŸ—‚ï¸Â MilestoneÂ 6Â â€”Â Refactor & PR
- [ ] **M6â€‘23 â€“ move all hacks to `vllm/engine/early_exit.py`; clean imports**
  - `tests/smoke_greedy.py` (layerskip=0 ensures noâ€‘regression)
  - `tests/smoke_skip.py` (goldÂ 32â€‘tok file; CI A100â€‘40Â GB)
- [ ] **M6â€‘24 â€“ unit test** (`tests/test_early_exit.py`: ensure identical tokens when gate forcedÂ `TAU=0`)
- [ ] **M6â€‘25 â€“ open PRÂ "earlyâ€‘exit selfâ€‘spec"** to upstream vLLM
- [ ] **M6â€‘26 â€“ add docs snippet** (`docs/performance/early_exit.md`)

### PR Checklist
 - [ ] attach Î”â€‘PPL table & tokens/s bar chart in description
 - [ ] add draft releaseâ€‘note snippet

---

## ðŸ“£Â MilestoneÂ 7Â â€”Â Blog & Broadcast
- [ ] **M7â€‘27 â€“ write `blog/early_exit_llama4.md`**  
  - intro, why raw head fails  
  - 150â€¯kâ€‘param fix diagram  
  - embed 10â€‘sec GIF (htop + live token stream) top of article
  - speed table (baseline vs skip vs LSQ vs selfâ€‘spec vs varH)  
  - locust latency chart
- [ ] **M7â€‘28 â€“ tweet thread + HN post** linking blog + PR

---

## ðŸš€Â Stretch Goals (postâ€‘merge)
- [ ] **SGâ€‘A â€“ autotuneâ€¯`LAYERSKIP` per prompt length**
- [ ] **SGâ€‘B â€“ bandit gate (Thompson)**
- [ ] **SGâ€‘C â€“ port to TensorRTâ€‘LLM engine**
- [ ] **SGâ€‘D â€“ hook into router project (`llm-highperf-router`) as "localâ€‘gpu" backend**

---

### ðŸ“ˆÂ Expected Metric Targets
| stage | tok/s | acceptâ€‘rate | Î”PPL | SM Util % |
|-------|-------|------------|------|-----------|
| baseline INT4 |Â â‰ˆâ€¯400 |Â â€“ |Â â€“ | ~85% |
| skipâ€¯12 + LSQ |Â â‰ˆâ€¯610 |Â 37â€¯% |Â +1.1 | ~70% |
| selfâ€‘spec (Ï„â€¯0.35) |Â â‰ˆâ€¯720 |Â ~40â€¯% |Â +0.15 | ~65% |
| varHâ€¯<â€¯5 | **â‰ˆâ€¯820** |Â ~50â€¯% |Â +0.18 | ~60% |

---

### ðŸ”‘Â Key Paths To Touch
```
vllm/
 â””â”€ engine/
     â”œâ”€ spec_decode.py   
tests/
 â””â”€ test_early_exit.py   # golden tokens
scripts/
 â”œâ”€ bench.sh
 â”œâ”€ ppl_eval.sh
 â”œâ”€ profile.sh
 â””â”€ locust_load.py
ee_head/
 â””â”€ ee_layer12_lsq.pt
```