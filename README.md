
---

## 0â€¯.Â Why does this repo exist?
Most openâ€‘source stacks chase raw kernel speed (vLLM, TGI) but leave **routing, caching and observability** bolted on as afterâ€‘thoughts.  
Here we build a **composable control layer** that:

* **Chooses** the best model (localâ€¯GPU, quantized, or API) per request  
* **Streams** tokens with subâ€‘20â€¯ms TTFT  
* **Experiments** with research ideas: prefill/decode split, selfâ€‘speculative decoding, semantic cache, costâ€‘aware batching  
* **Exposes** rich metrics so you can _see_ every win / regression  

We start tiny â†’ iterate â†’ land upstream PRs.

---

## 0.5 Setup

## 0.6 Cloud Setup with SkyPilot

We use [SkyPilot](https://github.com/skypilot-org/skypilot) to manage cloud resources, specifically GCP spot instances with H100 GPUs. This gives us:

* **Cost efficiency**: Spot instances are ~70% cheaper than on-demand
* **Auto-recovery**: SkyPilot automatically resubmits jobs after preemptions
* **Easy scaling**: One command to launch/stop clusters
* **File sync**: Automatic code synchronization
* **VS Code integration**: Easy remote development

### Quick Start

Run the following command:
```bash
sky launch -c llama4 infra/selfspec.yaml --env HF_TOKEN=$HF_TOKEN --env WANDB_API_KEY=$WANDB_API_KEY --idle-minutes-to-autostop 30
```

This will 
* Provision a GCP spot instance with the specified GPU.
* Set up the necessary environment, including syncing your local code to and from GCS.
* Inject the provided Hugging Face and WandB tokens as environment variables.
* Automatically stop the instance after 30 minutes of inactivity to save costs.


## 1â€¯.Â QuickÂ 90â€‘second demo

```bash
# 1. clone & env
git clone https://github.com/yourname/llm-highperf-router && cd llm-highperf-router
conda env create -f env/environment.yml && conda activate llama4

# 2. pull Llamaâ€‘4Â Scoutâ€‘17Bâ€‘16Eâ€¯(INT4)
python scripts/pull_model.py model

# 3. baseline greedy benchmark (8â€‘tok prompt, 16â€‘tok gen)
./scripts/bench.sh baseline_sanity 8 16 --seed 0

# 4. enable earlyâ€‘exit selfâ€‘spec decoder
./scripts/bench.sh spec_varH5 32 128 \
  --layerskip 12 \
  --ee_weight ee_head/ee_layer12_lsq.pt \
  --ee_gate varH,5 --seed 0
```

> âœ…Â Expected:Â **â‰ˆâ€¯820â€¯tok/s** (2.0â€¯Ã— baseline) on a singleâ€¯H100Â 80â€¯GB.  
> All core 109â€¯B weights stay frozen; only a 150â€¯kâ€‘param projection is learned once.

---

## 2â€¯.Â Repo layout

```
.
â”œâ”€â”€ env/                  # conda + pip requirement files
â”œâ”€â”€ model/                # symlink / HF snapshot (not committed)
â”œâ”€â”€ ee_head/              # tiny projection checkpoint
â”‚Â Â  â””â”€â”€ ee_layer12_lsq.pt
â”œâ”€â”€ scripts/
â”‚Â Â  â”œâ”€â”€ pull_model.py     # HF download helper
â”‚Â Â  â”œâ”€â”€ bench.sh          # wraps vLLM benchmark_engine.py
â”‚Â Â  â”œâ”€â”€ fit_lsq.py        # CPU LSQ projection fit
â”‚Â Â  â”œâ”€â”€ profile.sh        # Nsight profiler helper
â”‚Â Â  â”œâ”€â”€ ppl_eval.sh       # lmâ€‘eval harness wrapper
â”‚Â Â  â””â”€â”€ locust_load.py    # latency soak
â”œâ”€â”€ vllm/ (submodule)     # fork; see engine/early_exit.py patch
â”œâ”€â”€ tests/                # smoke + goldenâ€‘token tests
â””â”€â”€ TODO.md               # granular 30â€‘task checklist
```

---

## 3â€¯.Â Highâ€‘level architecture

```text
Client â”€â”¬â”€> FastAPI Gateway (stream)
        â”œâ”€> Locust Bench (HTTP)
        â•°â”€> bench.sh (gRPC engine)
                 â”‚
                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  vLLM Engine (fork)     â”‚  â€“ int4 weights
   â”‚  â”œâ”€ Earlyâ€‘Exit Draft    â”‚     (layers 0â€‘12, LSQ head)
   â”‚  â”œâ”€ Varâ€‘Entropy Gate    â”‚
   â”‚  â”œâ”€ Verify Layers 13â€‘48 â”‚
   â”‚  â•°â”€ Paged KV Cache      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
             GPU (H100)
```

---

## 4â€¯.Â Roadmap & Milestones

| ID | task | owner | status |
|----|------|-------|--------|
|M0â€‘03|bench script + baseline JSON|âœ…|
|M1â€‘05|`--layerskip` CLI + partial forward|ğŸŸ¡|
|M2â€‘09|LSQ projection fit (CPU) + saveÂ `.pt`|â¬œ|
|M3â€‘12|selfâ€‘spec verify loop + maxâ€‘prob gate|â¬œ|
|M4â€‘17|varâ€‘entropy gate sweep Ï„V=5|â¬œ|
|M5â€‘21|Locust soak (32 users, 10â€¯min)|â¬œ|
|M6â€‘23|refactor â†’ `early_exit.py`, unit tests|â¬œ|
|M6â€‘25|open PR to upstream vLLM|â¬œ|
|M7â€‘27|blog post + GIF demo||â¬œ|

*(full 30â€‘item checklist inÂ `TODO.md`)*

---

## 5â€¯.Â Benchmarking Cheatsheet

| command | what it does |
|---------|--------------|
|`./scripts/bench.sh baseline 32 128 --seed 0`|greedy baseline INT4|
|`./scripts/bench.sh skip12 32 128 --layerskip 12 --seed 0`|layerâ€‘skip only|
|`./scripts/bench.sh spec_varH5 32 128 --layerskip 12 --ee_weight ee_head/ee_layer12_lsq.pt --ee_gate varH,5`|full selfâ€‘spec pipeline|

`bench.sh` writes `results/*.json` (tokens/s, acceptâ€‘rate, SMÂ util).  
Use `python scripts/plot_results.py` to render the bar chart for your PR.

---

## 6â€¯.Â Target Metrics

| metric | baseline | goal |
|--------|----------|------|
|Timeâ€‘toâ€‘firstâ€‘token (32/128) |Â ~220â€¯ms |Â <120â€¯ms |
|Tokens/s |Â 400 |Â â‰¥â€¯800 |
|Î”Â Perplexity (ShareGPTÂ 1â€¯k) |Â â€” |Â â‰¤â€¯+0.18 |
|GPU SM util |Â <30â€¯% |Â >55â€¯% |
|Acceptâ€‘rate |Â 0â€¯% |Â ~50â€¯% |

---

## 8â€¯.Â Contributing

1. Fork, branch from `main`.  
2. Keep diffs â‰¤â€¯200â€¯LOC; add/Â update tests.  
3. `pre-commit run -a` must pass.  
4. Open PR; attach tokens/s chart + Î”Â PPL table, and provide a oneâ€‘liner changelog.

---

## 9â€¯.Â References

* AppleÂ (2024). *Recurrentâ€¯Drafter: singleâ€‘model speculative decoding.*  
* NVIDIAÂ TensorRTâ€‘LLM docs, `forward_partial` API.  
* LayerSkipÂ (2023). *Earlyâ€‘exit techniques for transformers.*  
* vLLM PRÂ #4630 â€“ speculative decoding stub.